# Core dependencies
#gradio==5.49.1
#gradio_client==1.13.3
#transformers-stream-generator==0.0.5
#git+https://github.com/huggingface/transformers.git@7aa888b7fa477d13153ffbfe107dfbd6c696014a
transformers
torch<2.9
torchvision<0.24
torchcodec
accelerate

# Optional dependency
# Uncomment the following line if you need flash-attn
# flash-attn

# GN

# needed for video pre-processing
av
# needed for qwen helpers
qwen-utils
qwen-vl-utils

# flash attention: install from binaries: https://github.com/Dao-AILab/flash-attention/releases
# wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp311-cp311-linux_x86_64.whl
# pip install ^
# that version needs
# pip install -U 'torch<2.9' 'torchvision<0.24'

# recommended by transformers warning during execution
